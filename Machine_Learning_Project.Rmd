---
title: "Qualitative Assessment of Weight Lifting Exercises"
author: "kurunaadan"
date: "Sunday, August 23, 2015"
output: html_document
---
## Overview

   Physical activity leads to a healthy life. It can reduce potential attacks of diseases like heart attack, type 2 diabetes and so on. Training in an effective way is important factor for a positive impact whereas mistakes during exercises would harm the individual. *Free weights* accounts for most of weight related training exercises. There are two ways to monitior the exercise regimen. One is employing a personal coach and the other is using on-body sensors and analyze the data obtained. It is possible to automatically assess the quality of execution of weight lifting exercises. In this project, we consider the data used by the following paper to find the most influential predictors. 

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.* 
   **Qualitative Activity Recognition of Weight Lifting Exercises.** Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)  Stuttgart, Germany: ACM SIGCHI, 2013.
   
   The data has 19622 observations taken from six individuals aged between 20-28 years, with little weight lifting experience. They were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 
   1) exactly according to the specification (Class A), 
   2) throwing the elbows to the front (Class B), 
   3) lifting the dumbbell only halfway (Class C), 
   4) lowering the dumbbell only halfway (Class D) and 
   5) throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 
4 classes correspond to common mistakes.  We use machine learning and pattern recognition techniques to detect mistakes and provide feedback on the quality of execution to the user.

```{r overview, echo=FALSE, warning=FALSE,message=FALSE}
# Load required libraries
library(caret)
library(doParallel)
library(mlbench)
library(pander)
library(kernlab)
library(gbm)
library(plyr)
library(class)


# Load required functions
source(file = "clean_NAs.R")
source(file = "change_to_numeric.R")

# Check if cleaned data is available. If available, skip and jump
raw.training.data <-  read.csv2(file = "pml-training.csv",sep = ",", header = TRUE,stringsAsFactors=FALSE)
num.obs <- nrow(raw.training.data)
```

## Exploratory Analysis
 
 The following table shows the first three observations with every attribute.
 
```{r exploratory_analysis, echo=FALSE, results='asis'}
pander(head(raw.training.data,3))

```

It can be seen that some varaiables like *kurtosis_picth_belt*, 
*kurtosis_yaw_belt*, *skewness_roll_belt* etc doesn't have any values at all.
And few other variables seems to have only NAs. So, we proceed to clean the data 
first.

## Cleaning Data 

```{r clean_data}
train.file <- "cleaned.rds"
if (!file.exists(train.file)) {
clean.training.data <- clean_NAs(raw.training.data)
clean.training.data <- change_to_numeric(clean.training.data, "kurtosis")
clean.training.data <- change_to_numeric(clean.training.data, "skewness")
clean.training.data <- change_to_numeric(clean.training.data, "gyros")
clean.training.data <- change_to_numeric(clean.training.data, "max")
clean.training.data <- change_to_numeric(clean.training.data, "min")
clean.training.data <- clean_NAs(clean.training.data)
clean.training.data <- change_to_numeric(clean.training.data, "roll_")
clean.training.data <- change_to_numeric(clean.training.data, "pitch_")
clean.training.data <- change_to_numeric(clean.training.data, "yaw_")
clean.training.data <- clean_NAs(clean.training.data)
clean.training.data$magnet_dumbbell_z <- as.integer(clean.training.data$magnet_dumbbell_z)
clean.training.data$magnet_forearm_y <- as.integer(clean.training.data$magnet_forearm_y)
clean.training.data$magnet_forearm_z <- as.integer(clean.training.data$magnet_forearm_z)

clean.training.data <- clean.training.data[,-seq(1:7)]

# Outcome variable
clean.training.data$classe <- as.factor(clean.training.data$classe)
saveRDS(object = clean.training.data,file = "cleaned.rds")
}
```

After cleaning the data, only 53 attributes remain out of 160, and we can do some preprocessing to reduce redundant ones further.

## Preprocessing of Data

```{r preprocess_data, results='asis'}
if (file.exists(train.file)) {

clean.training.data <- readRDS(file = "cleaned.rds")

# Find near zero variance predictors and eliminate them
nzv <- nearZeroVar(clean.training.data, saveMetrics= TRUE)
# Result: From the fourth column of nzv, it is understood that there are no 
# near-zero-variance variables

#Identifying Correlated Predictors
# The code chunk below shows the effect of removing descriptors with absolute 
# correlations above 0.75.
descrCor <- cor(clean.training.data[,-53])

highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
clean.training.data <- clean.training.data[,-highlyCorDescr]

#Find Linear Dependencies of columns
# The function findLinearCombos uses the QR decomposition of a matrix to enumerate
# sets of linear combinations (if they exist).
comboInfo <- findLinearCombos(clean.training.data[,-32])
# Results show that there are no linear combinations of columns

clean.training.data <- clean.training.data[complete.cases(clean.training.data),]
# All the 19622 rows are complete rows. no NAs
}
```

We calculated the correlation between variables and those who have absolute correlation values above 0.75 are discarded. Only 31 remained and further reduction by finding linear combinations didn't yield any useful results.

## Training Different Models

  Now, we choose three different "machine learning" models and train them on a 
small portion (approx. 10%) of data. The models chosen are **Learning Vector Quantization**, **Support Vector Machines with Radial Basis Function Kernel** 
and **Stochastic Gradient Boosting**

```{r different_models, results='asis',warning=FALSE, message=FALSE}

set.seed(32323)
x <- seq(1:num.obs)
s <- sample(x = x,size = 1000) 
training.data <- clean.training.data[s,]
remaining.data <- clean.training.data[-s,]

## Trying three models random lvq, svm and gbm

#control list
myControl <- trainControl(method="repeatedcv", number=10, repeats=3)

#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)

set.seed(32323)
model1_file <- "model1_file_v01.rds"
if (file.exists(model1_file)) {
    # Read the model in and assign it to a variable.
    model1 <- readRDS("model1_file_v01.rds")
} else {
    # Otherwise, run the training.
    model1 <- train(classe~., data = training.data, method='lvq', trControl=myControl)    
    saveRDS(object = model1,file = "model1_file_v01.rds")
}

set.seed(32323)
model2_file <- "model2_file_v01.rds"
if (file.exists(model2_file)) {
    # Read the model in and assign it to a variable.
    model2 <- readRDS("model2_file_v01.rds")
} else {
    # Otherwise, run the training.
    model2 <- train(classe~., data = training.data, method='svmRadial',trControl=myControl)    
    saveRDS(object = model2,file = "model2_file_v01.rds")
}

set.seed(32323)
model3_file <- "model3_file_v01.rds"
if (file.exists(model3_file)) {
    # Read the model in and assign it to a variable.
    model3 <- readRDS("model3_file_v01.rds")
} else {
    # Otherwise, run the training.
    model3 <- train(classe~., data = training.data, method='gbm', trControl=myControl,verbose = FALSE)    
    saveRDS(object = model3,file = "model3_file_v01.rds")
}

stopCluster(cl)

```

We do 10-fold cross validation repeating three times on the sample dataset of 
1000 observations. And we use parallization of the training part effectively 
using multiple processor cores (here it is 2)

## Choosing the Best Model

```{r best_model, echo=FALSE, warning=FALSE}
# collect resamples
results <- resamples(list(LVQ=model1, SVM=model2,GBM=model3))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
```

From the plot, it can be seen that **Stochastic Gradient Boosting** model 
performs the best. Now, let's do a full training.

## Training and Validation  

  We leave the used 1000 observations and consider the remaining data. Divide 
  70% of data for training and 30% for validation by the chosen best model GBM.
  
```{r test_data_prediction, results='markup', warning=FALSE, message=FALSE}
inTrain <- createDataPartition(y=remaining.data$classe, p=0.7, list=FALSE)
training.final <- remaining.data[inTrain,]
validation.final <- remaining.data[-inTrain,]

#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)

#control list
myControl1 <- trainControl(method='cv', number = 3)

best.model <- train(classe~., data = training.final, method='gbm', trControl=myControl1,verbose = FALSE)

stopCluster(cl)

# Confusion Matrix

Prediction <- predict(best.model, validation.final[,-32])
confusionMatrix(Prediction,validation.final$classe)
```

GBM model achieves 94% accuracy on the validation data. 

## Conclusion

```{r conclusion, results='markup', warning=FALSE, message=FALSE}
varImp(best.model)
```

    We can conclude that with only 31 variables, we could reach a reasonable 
 accuracy. Among them are  **magnet_belt_y**, **pitch_forearm** and 
 **yaw_belt ** are the highest accurate predictors.

